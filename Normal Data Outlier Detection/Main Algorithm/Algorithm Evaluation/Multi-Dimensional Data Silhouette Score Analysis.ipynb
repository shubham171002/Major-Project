{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc89aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01deb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9a6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import shapiro\n",
    "from scipy.stats import anderson\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyclustertend import hopkins\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fddd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.mcd import MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218e706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_KMeans(data,contamination=0.05,clustering=True):\n",
    "    data = np.array(data)\n",
    "    data_dimension = data.ndim\n",
    "    if data_dimension == 1:\n",
    "        # Single Dimensional Data\n",
    "        if anderson(data).statistic < anderson(data).critical_values[2]:\n",
    "            # data_distribution = 'normal'\n",
    "            sc = StandardScaler()\n",
    "            x_sc = sc.fit_transform(data.reshape(-1,1))\n",
    "            \n",
    "            if clustering == False:\n",
    "                outliers = x_sc[(x_sc > 3) | (x_sc < -3)]\n",
    "                labels = np.ones(len(data))\n",
    "                outlier_mask = np.isin(data, outliers)\n",
    "                labels[outlier_mask] = -1\n",
    "                return labels\n",
    "            else:\n",
    "                filtered_data = x_sc[(-3 <= x_sc) & (x_sc <= 3)]\n",
    "                hopkins_stats = hopkins(filtered_data,len(filtered_data)//10)\n",
    "                if hopkins_stats < 0.1:\n",
    "                    return ('Data can not be clustered')\n",
    "                else:\n",
    "                    k_values = range(2, 11)\n",
    "                    models = [KMeans(n_clusters=k, random_state=42,n_init='auto').fit(filtered_data.reshape(-1,1)) for k in k_values]\n",
    "                    labels = [model.labels_ for model in models]\n",
    "                    sil_score = [silhouette_score(filtered_data,label) for label in labels]\n",
    "                    optimal_k = k_values[np.argmax(sil_score)]\n",
    "                    \n",
    "                    model = KMeans(n_clusters=optimal_k,random_state=0,n_init='auto')\n",
    "                    model.fit(filtered_data.reshape(-1,1))\n",
    "                    labels = model.labels_\n",
    "                    cluster_label = -1 * np.ones_like(x_sc.flatten())\n",
    "                    cluster_label[np.unique(np.where((-3 <= x_sc) & (x_sc <= 3))[0])] = labels\n",
    "                    return cluster_label\n",
    "        \n",
    "        else:\n",
    "            # data_distribution = 'non_normal'\n",
    "            Q1 = np.quantile(data,0.25)\n",
    "            Q3 = np.quantile(data,0.75)\n",
    "            IQR = Q3-Q1\n",
    "            lw = Q1 - 1.5*IQR\n",
    "            rw = Q3 + 1.5*IQR\n",
    "            \n",
    "            if clustering == False:\n",
    "                outliers = data[(data > rw) | (data < lw)]\n",
    "                labels = np.ones(len(data))\n",
    "                outlier_mask = np.isin(data, outliers)\n",
    "                labels[outlier_mask] = -1\n",
    "                return labels\n",
    "            else:\n",
    "                filtered_data = data[(lw <= data) & (data <= rw)]\n",
    "                hopkins_stats = hopkins(filtered_data,len(filtered_data)//10)\n",
    "                if hopkins_stats < 0.1:\n",
    "                    return ('Data can not be clustered')\n",
    "                else:\n",
    "                    k_values = range(2, 11)\n",
    "                    models = [KMeans(n_clusters=k, random_state=42,n_init='auto').fit(filtered_data.reshape(-1,1)) for k in k_values]\n",
    "                    labels = [model.labels_ for model in models]\n",
    "                    sil_score = [silhouette_score(filtered_data,label) for label in labels]\n",
    "                    optimal_k = k_values[np.argmax(sil_score)]\n",
    "                    \n",
    "                    model = KMeans(n_clusters=optimal_k,random_state=0,n_init='auto')\n",
    "                    model.fit(filtered_data.reshape(-1,1))\n",
    "                    labels = model.labels_\n",
    "                    cluster_label = -1 * np.ones_like(x_sc.flatten())\n",
    "                    cluster_label[np.unique(np.where((-3 <= x_sc) & (x_sc <= 3))[0])] = labels\n",
    "                    return cluster_label\n",
    "    else:\n",
    "        # Multi Dimensional Data\n",
    "        if shapiro(data)[1] < 0.05:\n",
    "            #data_distribution = 'non_normal'\n",
    "            df = pd.DataFrame(data)\n",
    "            model = IsolationForest(contamination=contamination)\n",
    "            df['anomaly'] = model.fit_predict(df)\n",
    "            \n",
    "            if clustering == False:\n",
    "                return np.array(df['anomaly'])\n",
    "            else:\n",
    "                fil = df['anomaly'] == 1\n",
    "                filtered_data = df.loc[fil].iloc[:,:-1].values\n",
    "                hopkins_stats = hopkins(filtered_data,len(filtered_data)//10)\n",
    "                print(hopkins_stats)\n",
    "                if hopkins_stats < 0.1:\n",
    "                    return ('Data can not be clustered')\n",
    "                else:\n",
    "                    k_values = range(2, 11)\n",
    "                    models = [KMeans(n_clusters=k, random_state=42,n_init='auto').fit(filtered_data) for k in k_values]\n",
    "                    labels = [model.labels_ for model in models]\n",
    "                    sil_score = [silhouette_score(filtered_data,label) for label in labels]\n",
    "                    optimal_k = k_values[np.argmax(sil_score)]\n",
    "                    \n",
    "                    model = KMeans(n_clusters=optimal_k,random_state=42,n_init='auto')\n",
    "                    model.fit(filtered_data)\n",
    "                    labels = model.labels_\n",
    "                    df.loc[df['anomaly'] == -1,'label'] = -1\n",
    "                    df.loc[df['anomaly'] == 1,'label'] = labels\n",
    "                    return np.array(df['label'])\n",
    "                \n",
    "        else:\n",
    "            # data_distribution = 'normal'\n",
    "            df = pd.DataFrame(data)\n",
    "            model = MCD(contamination=contamination)\n",
    "            model.fit(data)\n",
    "            df['anomaly'] = model.predict(data)\n",
    "            df['anomaly'] = df['anomaly'].replace({0: 1, 1: -1})\n",
    "\n",
    "            if clustering == False:\n",
    "                return np.array(df['anomaly'])\n",
    "            else:\n",
    "                fil = df['anomaly'] == 1\n",
    "                filtered_data = df.loc[fil].iloc[:,:-1].values\n",
    "                hopkins_stats = hopkins(filtered_data,len(filtered_data)//10)\n",
    "                print(hopkins_stats)\n",
    "                if hopkins_stats < 0.1:\n",
    "                    return ('Data can not be clustered')\n",
    "                else:\n",
    "                    k_values = range(2, 11)\n",
    "                    models = [KMeans(n_clusters=k, random_state=42,n_init='auto').fit(filtered_data) for k in k_values]\n",
    "                    labels = [model.labels_ for model in models]\n",
    "                    sil_score = [silhouette_score(filtered_data,label) for label in labels]\n",
    "                    optimal_k = k_values[np.argmax(sil_score)]\n",
    "                    \n",
    "                    model = KMeans(n_clusters=optimal_k,random_state=42,n_init='auto')\n",
    "                    model.fit(filtered_data)\n",
    "                    labels = model.labels_\n",
    "                    df.loc[df['anomaly'] == -1,'label'] = -1\n",
    "                    df.loc[df['anomaly'] == 1,'label'] = labels\n",
    "                    return np.array(df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b78cf",
   "metadata": {},
   "source": [
    "### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f272c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A1     A2     A3      A4      A5      A6     A7  Target\n",
       "0  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070       0\n",
       "1  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210       0\n",
       "2  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155       0\n",
       "3  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055       0\n",
       "4  0.425  0.300  0.095  0.3515  0.1410  0.0775  0.120       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Outlier_Datasets/Dataset_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc8caa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "0    2730\n",
       "1      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
